{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf043f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain openai \n",
    "# pip install TextBlob\n",
    "# pip install --upgrade --quiet  langchain-openai tiktoken chromadb langchain\n",
    "# pip install langchain_experimental\n",
    "# pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53ade79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source :https://www.analyticsvidhya.com/blog/2023/06/how-to-automate-data-analysis-with-langchain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ddad436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "\n",
    "#setup the api key \n",
    "# os.environ['OPENAI_API_KEY1']=\"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\" #Julien key\n",
    "# os.environ['OPENAI_API_KEY2']=\"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\" #Haythem key\n",
    "# os.environ['OPENAI_API_KEY3']=\"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\" #Téo key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adbc1538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>rating</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>customer_location</th>\n",
       "      <th>comment_title</th>\n",
       "      <th>comment_content</th>\n",
       "      <th>comment_date</th>\n",
       "      <th>comment_date_of_experience</th>\n",
       "      <th>company_response</th>\n",
       "      <th>company_response_date</th>\n",
       "      <th>company_response_content</th>\n",
       "      <th>response_duration</th>\n",
       "      <th>company_id</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08ccc344-9d53-4b6e-ba01-935fc6538fcd</td>\n",
       "      <td>M M</td>\n",
       "      <td>5</td>\n",
       "      <td>Verified</td>\n",
       "      <td>US</td>\n",
       "      <td>Chelsey C has helped me…&amp; continues to…</td>\n",
       "      <td>Chelsey C has helped me…&amp; continues to help me...</td>\n",
       "      <td>2024-03-02</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-03-04</td>\n",
       "      <td>Thank you for sharing your experience with us!...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65f17384a149b0031323e939</td>\n",
       "      <td>Evergreen Credit Union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bc013c65-9b43-4c91-ac8a-3679e8a9ff17</td>\n",
       "      <td>Diana Dolstra</td>\n",
       "      <td>5</td>\n",
       "      <td>Redirected</td>\n",
       "      <td>US</td>\n",
       "      <td>Thanks to ECGU's debit/credit card fraud staff</td>\n",
       "      <td>I really appreciate the debit/credit fraud dep...</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-03-04</td>\n",
       "      <td>Thank you for leaving a review with us, Diana!...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65f17384a149b0031323e939</td>\n",
       "      <td>Evergreen Credit Union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98b71ebe-0860-4a3d-970c-b959749f9c9c</td>\n",
       "      <td>customer  Mary Fuller</td>\n",
       "      <td>5</td>\n",
       "      <td>Verified</td>\n",
       "      <td>US</td>\n",
       "      <td>The staff is courteous</td>\n",
       "      <td>The staff is courteous, respectful and knowled...</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>2024-02-20</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>Thank you for leaving a review, Mary! We're ha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65f17384a149b0031323e939</td>\n",
       "      <td>Evergreen Credit Union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87413598-a62c-45be-867f-d37d4c34db7d</td>\n",
       "      <td>Heidi Hoffman</td>\n",
       "      <td>5</td>\n",
       "      <td>Verified</td>\n",
       "      <td>US</td>\n",
       "      <td>Great people and banking experiences</td>\n",
       "      <td>I have known about Evergreen Credit Union sinc...</td>\n",
       "      <td>2024-02-22</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-03-04</td>\n",
       "      <td>Thank you, Heidi! We love hearing about famili...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>65f17384a149b0031323e939</td>\n",
       "      <td>Evergreen Credit Union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7d1cf9c4-828f-4c30-bc5c-d49e16bf76f5</td>\n",
       "      <td>customer</td>\n",
       "      <td>5</td>\n",
       "      <td>Verified</td>\n",
       "      <td>US</td>\n",
       "      <td>Jenn was so amazing</td>\n",
       "      <td>Jenn was so amazing. She really helped me thru...</td>\n",
       "      <td>2024-03-05</td>\n",
       "      <td>2024-02-27</td>\n",
       "      <td>True</td>\n",
       "      <td>2024-03-11</td>\n",
       "      <td>We're so glad Jenn was able to help you with y...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>65f17384a149b0031323e939</td>\n",
       "      <td>Evergreen Credit Union</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             comment_id          customer_name  rating  \\\n",
       "0  08ccc344-9d53-4b6e-ba01-935fc6538fcd                    M M       5   \n",
       "1  bc013c65-9b43-4c91-ac8a-3679e8a9ff17          Diana Dolstra       5   \n",
       "2  98b71ebe-0860-4a3d-970c-b959749f9c9c  customer  Mary Fuller       5   \n",
       "3  87413598-a62c-45be-867f-d37d4c34db7d          Heidi Hoffman       5   \n",
       "4  7d1cf9c4-828f-4c30-bc5c-d49e16bf76f5               customer       5   \n",
       "\n",
       "  verification_status customer_location  \\\n",
       "0            Verified                US   \n",
       "1          Redirected                US   \n",
       "2            Verified                US   \n",
       "3            Verified                US   \n",
       "4            Verified                US   \n",
       "\n",
       "                                    comment_title  \\\n",
       "0         Chelsey C has helped me…& continues to…   \n",
       "1  Thanks to ECGU's debit/credit card fraud staff   \n",
       "2                          The staff is courteous   \n",
       "3            Great people and banking experiences   \n",
       "4                             Jenn was so amazing   \n",
       "\n",
       "                                     comment_content comment_date  \\\n",
       "0  Chelsey C has helped me…& continues to help me...   2024-03-02   \n",
       "1  I really appreciate the debit/credit fraud dep...   2024-03-01   \n",
       "2  The staff is courteous, respectful and knowled...   2024-02-29   \n",
       "3  I have known about Evergreen Credit Union sinc...   2024-02-22   \n",
       "4  Jenn was so amazing. She really helped me thru...   2024-03-05   \n",
       "\n",
       "  comment_date_of_experience  company_response company_response_date  \\\n",
       "0                 2024-03-01              True            2024-03-04   \n",
       "1                 2024-03-01              True            2024-03-04   \n",
       "2                 2024-02-20              True            2024-02-29   \n",
       "3                 2024-02-21              True            2024-03-04   \n",
       "4                 2024-02-27              True            2024-03-11   \n",
       "\n",
       "                            company_response_content  response_duration  \\\n",
       "0  Thank you for sharing your experience with us!...                2.0   \n",
       "1  Thank you for leaving a review with us, Diana!...                3.0   \n",
       "2  Thank you for leaving a review, Mary! We're ha...                0.0   \n",
       "3  Thank you, Heidi! We love hearing about famili...               11.0   \n",
       "4  We're so glad Jenn was able to help you with y...                6.0   \n",
       "\n",
       "                 company_id            company_name  \n",
       "0  65f17384a149b0031323e939  Evergreen Credit Union  \n",
       "1  65f17384a149b0031323e939  Evergreen Credit Union  \n",
       "2  65f17384a149b0031323e939  Evergreen Credit Union  \n",
       "3  65f17384a149b0031323e939  Evergreen Credit Union  \n",
       "4  65f17384a149b0031323e939  Evergreen Credit Union  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the data\n",
    "df = pd.read_json('comments_to_analyse.json')\n",
    "# Initializing the agent \n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba87ede",
   "metadata": {},
   "source": [
    "# Script pour transferer un json ==> csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a2214a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully converted to CSV and JSON!\n"
     ]
    }
   ],
   "source": [
    "# Read JSON data from a file (replace 'data.json' with your actual file path)\n",
    "with open('comments_to_analyse.json', encoding='utf-8') as inputfile:\n",
    "    df = pd.read_json(inputfile)\n",
    "\n",
    "# Write the DataFrame to a CSV file (replace 'data.csv' with your desired output file path)\n",
    "df.to_csv('test.csv', encoding=\"utf-8\", index=False)\n",
    "df.to_json('test.json')\n",
    "print(\"JSON data successfully converted to CSV and JSON!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8291bfe",
   "metadata": {},
   "source": [
    "# Model 1: Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9579099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbr_of_batchs 200\n",
      "################################# => batch number: 0 <= #################################\n",
      "df_without_analyzed_comments size: 4605\n",
      "comment_counter: 0 & batch_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    2011ac81-e174-4fe3-8836-68e8ea20e4b8\n",
       "1    c94e4850-e688-4313-bf56-d1d0754bf9ad\n",
       "Name: comment_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################# => batch number: 1 <= #################################\n",
      "df_without_analyzed_comments size: 4605\n",
      "comment_counter: 2 & batch_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    7694a9a2-7ed8-4ca8-b77b-24278bc520f2\n",
       "3    0697c119-5590-476d-81f4-4a157681f5a9\n",
       "Name: comment_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################# => batch number: 2 <= #################################\n",
      "df_without_analyzed_comments size: 4605\n",
      "comment_counter: 4 & batch_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4    a16b0b66-d798-4ecd-abf2-222f57df14c5\n",
       "5    c9768028-4700-409d-80af-3e0d5ebcd337\n",
       "Name: comment_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################# => batch number: 3 <= #################################\n",
      "df_without_analyzed_comments size: 4605\n",
      "comment_counter: 6 & batch_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6    351caaa3-2570-4f40-9ed2-48d05fb8675d\n",
       "7    c30d3f98-7c46-4270-bec3-4a86e4dfff94\n",
       "Name: comment_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################# => batch number: 4 <= #################################\n",
      "df_without_analyzed_comments size: 4605\n",
      "comment_counter: 8 & batch_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8    38bad58a-2618-4545-8f88-4f599c45b5df\n",
       "9    8de5561b-2b64-4297-98af-1b58f22af5df\n",
       "Name: comment_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################# => batch number: 5 <= #################################\n",
      "df_without_analyzed_comments size: 4605\n",
      "comment_counter: 10 & batch_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10    9334a1d5-1d86-4355-8d55-0f1faf8f1d51\n",
       "11    2e434b15-d57f-4961-9d68-c8ebca5a0c8d\n",
       "Name: comment_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################# => batch number: 6 <= #################################\n",
      "df_without_analyzed_comments size: 4605\n",
      "comment_counter: 12 & batch_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12    b410e496-8e05-4f6f-b2ba-1627b01d9527\n",
       "13    23da8c3f-d56e-415a-8519-b1abd41a85bc\n",
       "Name: comment_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################# => batch number: 7 <= #################################\n",
      "df_without_analyzed_comments size: 4605\n",
      "comment_counter: 14 & batch_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14    38152ef9-a7b7-4440-b8e7-467c6ca400fe\n",
       "15    0198af04-7492-4383-aa2f-d69046208a52\n",
       "Name: comment_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################# => batch number: 8 <= #################################\n",
      "df_without_analyzed_comments size: 4605\n",
      "comment_counter: 16 & batch_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16    3ad3f713-a85c-4b97-9f46-c1668e40a948\n",
       "17    e0d92c3c-e30d-48d5-ae17-64f171146392\n",
       "Name: comment_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################# => batch number: 9 <= #################################\n",
      "df_without_analyzed_comments size: 4605\n",
      "comment_counter: 18 & batch_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18    a4c7408c-d451-4a4a-a4f6-5bbe9842d4c4\n",
       "19    45018c2c-478a-4dba-a82c-90cceb450d77\n",
       "Name: comment_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################# => batch number: 10 <= #################################\n",
      "df_without_analyzed_comments size: 4605\n",
      "comment_counter: 20 & batch_size: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20    e75fc0ae-5047-4dbd-ad72-9f1510f4aa59\n",
       "21    7cf04745-9136-468a-9a5a-713256454edf\n",
       "Name: comment_id, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END OF MAIN\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from langchain_experimental.agents import create_pandas_dataframe_agent \n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "from langchain.chains.llm import LLMChain\n",
    "# from langchain.prompts import PromptTemplate\n",
    "from langchain import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from math import *\n",
    "import time\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "def print_comment_id(df_ids):\n",
    "    display(df_ids['comment_id'])\n",
    "\n",
    "def clean_json_file(filename):\n",
    "    '''\n",
    "    open the filename.json and clean the content \n",
    "    '''\n",
    "    # Open the file in write mode, which automatically deletes all its content\n",
    "    with open(filename, mode=\"w\", encoding=\"utf-8\") as json_file:\n",
    "        json_file.write(\"[]\")  # Écrire un objet vide dans le fichier\n",
    "\n",
    "# def add_data_to_json(filename, data):\n",
    "#     '''\n",
    "#     Add data to the .json filename in parameter\n",
    "#     '''\n",
    "#     # Load the existing JSON file\n",
    "#     with open(filename, 'r') as json_file:\n",
    "#      json_content = json.load(json_file)\n",
    "#     # Add the new documents to the existing data\n",
    "#     for comment in data:\n",
    "#         json_comment = comment.to_json()\n",
    "#         print(f'json_comment: {json_comment}')\n",
    "#         json_content.append(json_comment)\n",
    "#     # Write the updated data in the JSON file\n",
    "#     with open(filename, mode=\"w\", encoding=\"utf-8\") as json_file:\n",
    "#         json.dump(json_content, json_file , indent=2) \n",
    "\n",
    "################################### add_df_to_json ##################################### \n",
    "def add_df_to_json(filename, df_data):\n",
    "    '''\n",
    "    Add data to the .json filename in parameter\n",
    "    '''\n",
    "    print(f'-------------------------> add_data_to_json')\n",
    "    # Load the existing JSON file\n",
    "    with open(filename, 'r') as json_file:\n",
    "     json_content = json.load(json_file)\n",
    "        \n",
    "    # print('json_content')\n",
    "    # print(json_content)\n",
    "    print(f'len(df_data): {len(df_data)}')\n",
    "    # display(df_data)\n",
    "    print(f'df_data.index: {df_data.index}')\n",
    "    # Add the new documents to the existing data\n",
    "    for i in range(len(df_data)):\n",
    "        # print(f'dataframe {i}')\n",
    "        # display(df_data.iloc[i, ])\n",
    "        dict_comment = df_data.iloc[i,].to_dict()\n",
    "        # print('dict')\n",
    "        # print(dict_comment,'\\n')\n",
    "        json_content.append(dict_comment)\n",
    "\n",
    "    # print('json_content')\n",
    "    # print(json_content)\n",
    "    # Write the updated data in the JSON file\n",
    "    with open(filename, mode=\"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(json_content, json_file , indent=2) \n",
    "################################### /add_df_to_json ##################################### \n",
    "\n",
    "\n",
    "\n",
    "############################### remove_analyzed_comments ################################ \n",
    "def remove_analyzed_comments(df_without_analyzed_comments, df_output):\n",
    "    print(f'-------------------------> remove_analyzed_comments')\n",
    "    # display(df_without_analyzed_comments)\n",
    "    # print('')\n",
    "    # display(df_output)\n",
    "    # On récupére les comment_id\n",
    "    comments_id = df_output['comment_id'].unique()\n",
    "    # print(comments_id)\n",
    "    # Pour chaque comment_id on va supprimer les comments dans df\n",
    "    print('len df_without_analyzed_comments: ',len(df_without_analyzed_comments))\n",
    "    for id in comments_id:\n",
    "        print(id)\n",
    "        df_without_analyzed_comments = df_without_analyzed_comments.drop(df_without_analyzed_comments[df_without_analyzed_comments['comment_id'] == id].index)\n",
    "    print('len df_without_analyzed_comments: ',len(df_without_analyzed_comments))\n",
    "    \n",
    "    # On enregistre de nouveau dans le fichier json\n",
    "    clean_json_file('comments_to_analyse.json')\n",
    "    add_df_to_json('comments_to_analyse.json',df_without_analyzed_comments)\n",
    "    \n",
    "    return df_without_analyzed_comments\n",
    "\n",
    "############################### /remove_analyzed_comments ###############################  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################## save_openAI_output_into_csv ############################## \n",
    "def save_openAI_output_into_json(output_openAI):\n",
    "    print(f'-------------------------> save_openAI_output_into_csv')\n",
    "    # On initialise le dataframe qui récupérera le full output\n",
    "    columns = ['comment_id','company_id','Category','Nature','Analysis']\n",
    "    df_responses =pd.DataFrame(data=None, index=None, columns=columns, dtype=None, copy=None)\n",
    "\n",
    "    # On transforme le str contenu dans ['text'] en dataframe puis on le clean avant de l'ajouter dans df_responses\n",
    "    for output in output_openAI:\n",
    "        \n",
    "        # On transform le string \"output\" en dictionnaire\n",
    "        dict = json.loads(output)\n",
    "\n",
    "        # On transforme le dictionnaire en dataframe\n",
    "        if \"output\" in dict:\n",
    "            df_comment = json_normalize(dict['output'])\n",
    "        elif \"categories\" in dict:\n",
    "            df_comment = json_normalize(dict['categories'])\n",
    "        elif \"category\" in dict:\n",
    "            df_comment = json_normalize(dict['category'])\n",
    "        elif \"Categories\" in dict:\n",
    "            df_comment = json_normalize(dict['Categories'])\n",
    "        elif \"Category\" in dict:\n",
    "            df_comment = json_normalize(dict['Category'])\n",
    "        else:\n",
    "            df_comment = json_normalize(dict) \n",
    "            print(\"----> dataframe without ouput\")\n",
    "            print(df_comment,\"\\n\")\n",
    "            raise ValueError(\"OpenAI invented a key, we need to add the key and reload the script \")\n",
    "            \n",
    "        \n",
    "        \n",
    "        # df_comment = json_normalize(dict['output'])\n",
    "\n",
    "        # On ajoute la colonne company_id, comment_id et on ordonne les colonnes\n",
    "        df_comment = df_comment.assign(company_id=dict['company_id'],comment_id=dict['comment_id'])\n",
    "        df_comment = df_comment[columns]\n",
    "    \n",
    "        # # On enleve les lignes ou l'IA a quand même indiqué une catégorie mais avec rien dedans\n",
    "        df_comment = df_comment.drop(df_comment[(df_comment['Nature'] == \"Not mentioned\") | (df_comment['Nature'] == \"not mentioned\")].index)\n",
    "\n",
    "        # # On ajoute à df_responses\n",
    "        df_responses  = pd.concat([df_responses, df_comment],ignore_index=True, sort=False)\n",
    "        \n",
    "    print(\"-----output----\")\n",
    "    # print_comment_id(df_responses)\n",
    "    display(df_responses)\n",
    "    \n",
    "    # append data frame to CSV file\n",
    "    # df_responses.to_csv('comments_analyzed.csv', mode='a', index=False, header=False)\n",
    "    add_df_to_json('comments_analyzed.json',df_responses)\n",
    "    # df_responses.to_json('comments_analyzed.json', mode='a', orient='records', lines=True)\n",
    "    print(\"-----output saved to json----\")\n",
    "    \n",
    "    return df_responses\n",
    "############################## /save_openAI_output_into_csv ##############################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################## analyse_comment_with_openIA ##############################    \n",
    "# @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def analyse_comment_with_openIA(df_batch, list_llm_chain):\n",
    "    print(f'-------------------------> analyse_comment_with_openIA')\n",
    "    print(f'len(df_batch): {len(df_batch)}')\n",
    "\n",
    "    output_final = []\n",
    "    llm_number = 0 # Compteur pour alterner les llm_chain\n",
    "    # On parcours df_batch\n",
    "    for i in df_batch.index:\n",
    "        # Prepare the input for the chain\n",
    "        input = {'text': df_batch.loc[i, \"comment_content\"], 'comment_id':df_batch.loc[i,'comment_id'], 'company_id':df_batch.loc[i,'company_id']}\n",
    "        \n",
    "        # On détermine quel llm_chain doit être choisi car on alterne entre chacune\n",
    "        if llm_number == 0 :\n",
    "            print(\"llm_chain_clients1\")\n",
    "        elif llm_number == 1 :\n",
    "            print(\"llm_chain_clients2\")\n",
    "        elif llm_number == 2 :\n",
    "            print(\"llm_chain_haythem\")\n",
    "        \n",
    "        output = list_llm_chain[llm_number].run(input)\n",
    "        \n",
    "        \n",
    "\n",
    "        # On ajuste la valeur de llm_number avec une remise à 0 au cas ou on atteint la limite du batch size\n",
    "        # Ne devrait jamais arriver car le batch size ne doit jamais dépasser le nombre d'api_key\n",
    "        llm_number += 1\n",
    "        if llm_number == len(df_batch) : \n",
    "            llm_number = 0\n",
    "\n",
    "        print(output,\"\\n\")\n",
    "        output_final.append(output)\n",
    "\n",
    "    return output_final\n",
    "############################## /analyse_comment_with_openIA ##############################   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################## MAIN ##############################\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"\n",
    "As a marketing expert, you will analyse for each of these categories (delimited by <cat> </cat>) the following customer feedback: \"{text}\".\n",
    "Each category analysis will begin by the nature of the comment (Is it positive, negative, not mentioned) and then concisely explain why but only if it's positive or negative.\n",
    "The output should be in a JSON format and you will label it with its comment_id: \"{comment_id}\" and its company_id: \"{company_id}\".\n",
    "\n",
    "<cat>\n",
    "    Usability,\n",
    "    Speed/Performance,\n",
    "    Pricing,\n",
    "    Customer Service,\n",
    "    Product Quality,\n",
    "    Billing/Invoicing,\n",
    "    Delivery/Shipping,\n",
    "    Communication,\n",
    "    Returns/Refunds,\n",
    "    Product Features/Functionality\n",
    "</cat>\n",
    "\n",
    "<json_format>\n",
    "output: [\n",
    "    Category: Category_name,\n",
    "    Nature: answer,\n",
    "    Analysis: answer\n",
    "    ]\n",
    "</json_format>\n",
    "\"\"\"\n",
    "# # Define prompt 2\n",
    "# prompt_template2 = \"\"\"\n",
    "# As a marketing expert, you will analyse the following customer feedback: \"{text}\".\n",
    "# You will identify which categories this feedback relates to and for each category the analysis will begin by the nature of the comment (Is it positive, negative, not mentioned) and then concisely explain why but only if it's positive or negative.\n",
    "# The output should be in a JSON format and you will label it with its comment_id: \"{comment_id}\" and its company_id: \"{company_id}\"\n",
    "\n",
    "# <json_format>\n",
    "# [\n",
    "#     output: [\n",
    "#         comment_id: comment_id\n",
    "#         company_id: company_id\n",
    "#         Category: Category_name,\n",
    "#         Nature: answer,\n",
    "#         Analysis: answer\n",
    "#         ]\n",
    "# ]\n",
    "# </json_format>\n",
    "# \"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "list_llm_chain = []\n",
    "\n",
    "# Define LLM chain for each api_key\n",
    "llm_julien = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\",openai_api_key=\"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\")\n",
    "llm_chain_clients1 = LLMChain(llm=llm_julien, prompt=prompt)\n",
    "list_llm_chain.append(llm_chain_clients1)\n",
    "\n",
    "llm_teo = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\",openai_api_key=\"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\")\n",
    "llm_chain_clients2 = LLMChain(llm=llm_teo, prompt=prompt)\n",
    "list_llm_chain.append(llm_chain_clients2)\n",
    "\n",
    "# llm_haythem = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\",openai_api_key=\"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\")\n",
    "# llm_chain_haythem = LLMChain(llm=llm_haythem, prompt=prompt)\n",
    "# list_llm_chain.append(llm_chain_haythem)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# On récupére le json de comment à analyser\n",
    "df = pd.read_json('comments_to_analyse.json')\n",
    "# on créé le json sur lequel on va venir retirer les commentaires qui ont été analysé avant de l'enregistré\n",
    "df_without_analyzed_comments = df#.drop['company_id']\n",
    "\n",
    "# Variable servant à éviter l'erreur du rate limit\n",
    "rate_limit_per_minute = 3\n",
    "rate_limit_per_day = 200\n",
    "batch_size = 2 # taille du batch = nb api_key\n",
    "\n",
    "# initialisations des compteurs pour envoyer les requetes par batch\n",
    "length_df = len(df) # length of df\n",
    "nbr_of_batchs = ceil(length_df/batch_size) # On calcul le nombre de batch à l'arrondi supérieur et on le ramene en dessous de 200 si >\n",
    "if nbr_of_batchs > 200 : nbr_of_batchs = 200\n",
    "batch_counter = 1 #compteur pour debugging\n",
    "comment_counter = 0 #compteur pour debugging\n",
    "\n",
    "# Calculate the delay based on your rate limit\n",
    "delay_in_seconds = 60.0 / rate_limit_per_minute\n",
    "\n",
    "print('nbr_of_batchs',nbr_of_batchs)\n",
    "for i in range(nbr_of_batchs):\n",
    "    print(f'################################# => batch number: {i} <= #################################')\n",
    "    print(f'df_without_analyzed_comments size: {len(df_without_analyzed_comments)}')\n",
    "    print(f'comment_counter: {comment_counter} & batch_size: {batch_size}')\n",
    "    \n",
    "    # On récupére les comments du batch\n",
    "    df_batch = df.iloc[comment_counter:comment_counter+batch_size, ]\n",
    "    # display(df_batch)\n",
    "    print_comment_id(df_batch)\n",
    "\n",
    "    # On analyse les X comments et on récupére l'output de l'analyse\n",
    "    # output_openAI = analyse_comment_with_openIA(df_batch, list_llm_chain)\n",
    "\n",
    "    # On sauvegarde l'analyse dans le fichier comments_analyzed.csv\n",
    "    # df_output = save_openAI_output_into_json(output_openAI)\n",
    "    \n",
    "    # On enleve les commentaires analysé de df_without_analyzed_comments puis on écrit par dessus le fichier comments.json\n",
    "    # df_without_analyzed_comments = remove_analyzed_comments(df_without_analyzed_comments, df_output)\n",
    "    \n",
    "    # On incrémente batch_counter et comment_counter\n",
    "    batch_counter += 1\n",
    "    comment_counter += batch_size\n",
    "    \n",
    "    if i == 10:\n",
    "        break\n",
    "        \n",
    "    # Sleep for the delay\n",
    "    # print(f'Sleep for: {delay_in_seconds} ...')\n",
    "    # time.sleep(delay_in_seconds)\n",
    "\n",
    "print(\"END OF MAIN\")\n",
    "############################## /MAIN ##############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
